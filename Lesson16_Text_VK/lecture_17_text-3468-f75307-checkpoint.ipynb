{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='otus.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Николенко, Кадурин, Архангельская. **Глубокое обучение. Погружение в мир нейронных сетей**. Глава 7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какие задачи можно решать, обрабатывая текст?\n",
    "\"Мама мыла раму, и теперь она блестит\"  \n",
    "\"Мама мыла раму, и теперь она сильно устала\"  \n",
    "\n",
    "\"Кубок не помещался в чемодан, потому что он был слишком велик. Что именно было слишком велико, чемодан или кубок?\"\n",
    "\n",
    "http://commonsensereasoning.org/winograd.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. синтаксические задачи\n",
    "  * разметка по частям речи и по морфологическим признакам\n",
    "  * деление слов в тексте на морфемы (суффикс, приставка и пр.)\n",
    "  * стемминг, лемматизация (?)\n",
    "  * деление на предложения (инициалы и сокращения) и слова (китайский язык)\n",
    "  * поиск имен и названий в тексте - сущностей\n",
    "  * разрешение смысла слов в заданном контексте (замок)\n",
    "  * построить синтаксическое дерево\n",
    "  * определение того, к каким другим объектам относится слово\n",
    "2. задачи на понимание текста, в которых есть \"учитель\"\n",
    "  * предсказание следующего символа\n",
    "  * информационный поиск\n",
    "  * анализ тональности\n",
    "  * выделение отношений и фактов\n",
    "  * ответы на вопросы\n",
    "3. понимание и порождение текста (оценка качества?)\n",
    "  * порождение текста\n",
    "  * автоматическое реферирование\n",
    "  * машинный перевод\n",
    "  * диалоговые модели (чат-бот)\n",
    "  \n",
    "Косвенные задачи:\n",
    "  * описание изображения\n",
    "  * распознавание речи\n",
    "  \n",
    "**Задачи бизнеса**:\n",
    "  * распознавание речи (помощник)\n",
    "  * чат-бот (замена техподдержки в решении большинства вопросов)\n",
    "  * поиск точного ответа на вопрос в базе документов (например, база стандартов)\n",
    "  * оценка мнения в социальных сетях о продукте\n",
    "  * ... (ваши варианты?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()  # download lots of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# От текста к простым моделям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение на токены\n",
    "**Def.**  \n",
    "разбиение последовательности символов на части (токены), возможно, исключая из рассмотрения некоторые символы  \n",
    "Наивный подход: разделить строку пробелами и выкинуть знаки препинания  \n",
    "\n",
    "\n",
    "*Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.*  \n",
    "\n",
    "\n",
    "**Проблемы:**  \n",
    "* my.email@mail.ru, 127.0.0.1\n",
    "* С++, C#\n",
    "* York University vs New York University\n",
    "* Зависимость от языка (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "Альтернатива: n-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Трисия :: любила :: Нью :: - :: Йорк :: , :: поскольку ::\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "s = u'Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.'\n",
    "for t in tokenizer.tokenize(s)[:7]: \n",
    "    print t + \" ::\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm blue, da ba dee da ba doo...\n"
     ]
    }
   ],
   "source": [
    "from ftfy import fix_text\n",
    "print(fix_text(u'\\001\\033[36;44mI&#x92;m blue, da ba dee da ba doo&#133;\\033[0m', normalization='NFKC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стоп-слова\n",
    "**Def.**  \n",
    "Наиболее частые слова в языке, не содержащие никакой информации о содержании текста\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и в во не что он на я с со как а то все она так его но да ты\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print ' '.join(stopwords.words('russian')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема: “To be or not to be\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация\n",
    "**Def.**  \n",
    "Приведение токенов к единому виду для того, чтобы избавиться от поверхностной разницы в написании  \n",
    "\n",
    "Подходы  \n",
    "* сформулировать набор правил, по которым преобразуется токен  \n",
    "Нью-Йорк → нью-йорк → ньюйорк → ньюиорк\n",
    "* явно хранить связи между токенами (WordNet – Princeton)  \n",
    "машина → автомобиль, Windows 6→ window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нью-йорк\n"
     ]
    }
   ],
   "source": [
    "s = u'Нью-Йорк'\n",
    "s1 = s.lower()\n",
    "print s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюйорк\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s2 = re.sub(ur\"\\W\", \"\", s1, flags=re.U)\n",
    "print s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюиорк\n"
     ]
    }
   ],
   "source": [
    "s3 = re.sub(ur\"й\", u\"и\", s2, flags=re.U)\n",
    "print s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг и Лемматизация\n",
    "**Def.**  \n",
    "Приведение грамматических форм слова и однокоренных слов к единой основе (lemma):\n",
    "* Stemming – с помощью простых эвристических правил\n",
    "  * Porter (Cambridge – 1980)\n",
    "        5 этапов, на каждом применяется набор правил, таких как\n",
    "            sses → ss (caresses → caress)\n",
    "            ies → i (ponies → poni)\n",
    "\n",
    "  * Lovins (1968)\n",
    "  * Paice (1990)\n",
    "  * другие\n",
    "* Lemmatization – с использованием словарей и морфологического анализа\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\n",
      "stem\n",
      "авиац\n",
      "национальн\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "s = PorterStemmer()\n",
    "print s.stem('Tokenization')\n",
    "print s.stem('stemming')\n",
    "\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "r = RussianStemmer()\n",
    "print r.stem(u'Авиация')\n",
    "print r.stem(u'национальный')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдение**  \n",
    "для сложных языков лучше подходит лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word=u'\\u0437\\u0430\\u043c\\u043e\\u043a', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form=u'\\u0437\\u0430\\u043c\\u043e\\u043a', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, u'\\u0437\\u0430\\u043c\\u043e\\u043a', 139, 0),))\n",
      "замок замок\n",
      "Parse(word=u'\\u0437\\u0430\\u043c\\u043e\\u043a', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form=u'\\u0437\\u0430\\u043c\\u043e\\u043a', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, u'\\u0437\\u0430\\u043c\\u043e\\u043a', 139, 3),))\n",
      "замок замок\n",
      "Parse(word=u'\\u0437\\u0430\\u043c\\u043e\\u043a', tag=OpencorporaTag('VERB,perf,intr masc,sing,past,indc'), normal_form=u'\\u0437\\u0430\\u043c\\u043e\\u043a\\u043d\\u0443\\u0442\\u044c', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, u'\\u0437\\u0430\\u043c\\u043e\\u043a', 730, 1),))\n",
      "замок замокнуть\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for i in morph.parse(u'замок'):\n",
    "    print i\n",
    "    print i.word, i.normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Представление документов\n",
    "**Boolean Model.** Присутствие или отсутствие слова в документе  \n",
    "**Bag of Words.** Порядок токенов не важен  \n",
    "\n",
    "*Погода была ужасная, принцесса была прекрасная.\n",
    "Или все было наоборот?*\n",
    "\n",
    "Координаты\n",
    "* Мультиномиальные: количество токенов в документе\n",
    "* Числовые: взвешенное количество токенов в документе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.,  1.],\n",
       "       [ 0.,  1.,  3.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "v.fit(D)\n",
    "X = v.transform(D)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  4.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform({'foo': 4, 'unseen_feature': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({',': 1, '.': 1, '40': 1, 'mr': 1, 'president': 1, 'thank': 1}),\n",
       " Counter({\"'\": 1,\n",
       "          ',': 3,\n",
       "          ':': 1,\n",
       "          'agree': 1,\n",
       "          'auspicious': 1,\n",
       "          'european': 1,\n",
       "          'madam': 1,\n",
       "          'needs': 1,\n",
       "          'outcome': 1,\n",
       "          'president': 1,\n",
       "          'prospects': 2,\n",
       "          'recognise': 1,\n",
       "          'turkey': 2}),\n",
       " Counter({',': 2,\n",
       "          '.': 1,\n",
       "          'agenda': 1,\n",
       "          'early': 1,\n",
       "          'express': 1,\n",
       "          'firstly': 1,\n",
       "          'high': 1,\n",
       "          'important': 1,\n",
       "          'including': 1,\n",
       "          'issue': 1,\n",
       "          'like': 1,\n",
       "          'madam': 1,\n",
       "          'president': 1,\n",
       "          'representative': 1,\n",
       "          'sincerest': 1,\n",
       "          'stage': 1,\n",
       "          'thanks': 1,\n",
       "          'would': 1})]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "docs = [\n",
    "    \"Thank 40 you, Mr President.\",\n",
    "    \"Madam President, I agree and recognise Turkey's European prospects, but if these prospects are to have an auspicious outcome, Turkey needs to:\",\n",
    "    \"Madam President, firstly, I would like to express my sincerest thanks to the High Representative for including this important issue in the agenda at such an early stage.\",\n",
    "]\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "stopwords_eng = stopwords.words()\n",
    "\n",
    "document_bags = list()\n",
    "\n",
    "for d in docs:\n",
    "    bag = Counter()\n",
    "    text = d.lower()\n",
    "\n",
    "    for t in tokenizer.tokenize(text):     \n",
    "        if t in stopwords_eng:\n",
    "            continue\n",
    "            \n",
    "        bag[t] += 1\n",
    "    document_bags.append(bag)\n",
    "    \n",
    "document_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 30)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(document_bags)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " ',',\n",
       " '.',\n",
       " ':',\n",
       " 'agenda',\n",
       " 'agree',\n",
       " 'auspicious',\n",
       " 'early',\n",
       " 'european',\n",
       " 'express',\n",
       " 'firstly',\n",
       " 'high',\n",
       " 'important',\n",
       " 'including',\n",
       " 'issue',\n",
       " 'like',\n",
       " 'madam',\n",
       " 'mr',\n",
       " 'needs',\n",
       " 'outcome',\n",
       " 'president',\n",
       " 'prospects',\n",
       " 'recognise',\n",
       " 'representative',\n",
       " 'sincerest',\n",
       " 'stage',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'turkey',\n",
       " 'would']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0],\n",
       "        [0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 2, 0, 1, 2, 0, 1, 0]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(docs).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'40': 0,\n",
       " u'agenda': 1,\n",
       " u'agree': 2,\n",
       " u'an': 3,\n",
       " u'and': 4,\n",
       " u'are': 5,\n",
       " u'at': 6,\n",
       " u'auspicious': 7,\n",
       " u'but': 8,\n",
       " u'early': 9,\n",
       " u'european': 10,\n",
       " u'express': 11,\n",
       " u'firstly': 12,\n",
       " u'for': 13,\n",
       " u'have': 14,\n",
       " u'high': 15,\n",
       " u'if': 16,\n",
       " u'important': 17,\n",
       " u'in': 18,\n",
       " u'including': 19,\n",
       " u'issue': 20,\n",
       " u'like': 21,\n",
       " u'madam': 22,\n",
       " u'mr': 23,\n",
       " u'my': 24,\n",
       " u'needs': 25,\n",
       " u'outcome': 26,\n",
       " u'president': 27,\n",
       " u'prospects': 28,\n",
       " u'recognise': 29,\n",
       " u'representative': 30,\n",
       " u'sincerest': 31,\n",
       " u'stage': 32,\n",
       " u'such': 33,\n",
       " u'thank': 34,\n",
       " u'thanks': 35,\n",
       " u'the': 36,\n",
       " u'these': 37,\n",
       " u'this': 38,\n",
       " u'to': 39,\n",
       " u'turkey': 40,\n",
       " u'would': 41,\n",
       " u'you': 42}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Количество вхождений слова $t$ в документе $d$\n",
    "$$\n",
    "TF_{t,d} = term\\!\\!-\\!\\!frequency(t, d)\n",
    "$$\n",
    "Количество документов из $N$ возможных, где встречается $t$\n",
    "$$\n",
    "DF_t = document\\!\\!-\\!\\!fequency(t)\n",
    "$$\n",
    "$$\n",
    "IDF_t = inverse\\!\\!-\\!\\!document\\!\\!-\\!\\!frequency(t) = \\log \\frac{N}{DF_t}\n",
    "$$\n",
    "TF-IDF\n",
    "$$\n",
    "TF\\!\\!-\\!\\!IDF_{t,d} = TF_{t,d} \\times IDF_t\n",
    "$$\n",
    "\n",
    "Оценивает важность слова в контексте документа, являющегося частью корпуса\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.47952794,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.47952794,  0.        ,\n",
       "          0.        ,  0.        ,  0.28321692,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.47952794,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.47952794],\n",
       "        [ 0.        ,  0.        ,  0.20489728,  0.15582966,  0.20489728,\n",
       "          0.20489728,  0.        ,  0.20489728,  0.20489728,  0.        ,\n",
       "          0.20489728,  0.        ,  0.        ,  0.        ,  0.20489728,\n",
       "          0.        ,  0.20489728,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.15582966,  0.        ,  0.        ,\n",
       "          0.20489728,  0.20489728,  0.12101563,  0.40979456,  0.20489728,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.20489728,  0.        ,  0.31165933,\n",
       "          0.40979456,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.18959527,  0.        ,  0.14419209,  0.        ,\n",
       "          0.        ,  0.18959527,  0.        ,  0.        ,  0.18959527,\n",
       "          0.        ,  0.18959527,  0.18959527,  0.18959527,  0.        ,\n",
       "          0.18959527,  0.        ,  0.18959527,  0.18959527,  0.18959527,\n",
       "          0.18959527,  0.18959527,  0.14419209,  0.        ,  0.18959527,\n",
       "          0.        ,  0.        ,  0.11197802,  0.        ,  0.        ,\n",
       "          0.18959527,  0.18959527,  0.18959527,  0.18959527,  0.        ,\n",
       "          0.18959527,  0.37919054,  0.        ,  0.18959527,  0.28838418,\n",
       "          0.        ,  0.18959527,  0.        ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "features = vectorizer.fit_transform(docs).todense()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'40': 0,\n",
       " u'agenda': 1,\n",
       " u'agree': 2,\n",
       " u'an': 3,\n",
       " u'and': 4,\n",
       " u'are': 5,\n",
       " u'at': 6,\n",
       " u'auspicious': 7,\n",
       " u'but': 8,\n",
       " u'early': 9,\n",
       " u'european': 10,\n",
       " u'express': 11,\n",
       " u'firstly': 12,\n",
       " u'for': 13,\n",
       " u'have': 14,\n",
       " u'high': 15,\n",
       " u'if': 16,\n",
       " u'important': 17,\n",
       " u'in': 18,\n",
       " u'including': 19,\n",
       " u'issue': 20,\n",
       " u'like': 21,\n",
       " u'madam': 22,\n",
       " u'mr': 23,\n",
       " u'my': 24,\n",
       " u'needs': 25,\n",
       " u'outcome': 26,\n",
       " u'president': 27,\n",
       " u'prospects': 28,\n",
       " u'recognise': 29,\n",
       " u'representative': 30,\n",
       " u'sincerest': 31,\n",
       " u'stage': 32,\n",
       " u'such': 33,\n",
       " u'thank': 34,\n",
       " u'thanks': 35,\n",
       " u'the': 36,\n",
       " u'these': 37,\n",
       " u'this': 38,\n",
       " u'to': 39,\n",
       " u'turkey': 40,\n",
       " u'would': 41,\n",
       " u'you': 42}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank 40 you, Mr President.',\n",
       " \"Madam President, I agree and recognise Turkey's European prospects, but if these prospects are to have an auspicious outcome, Turkey needs to:\",\n",
       " 'Madam President, firstly, I would like to express my sincerest thanks to the High Representative for including this important issue in the agenda at such an early stage.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.        ],\n",
       "        [ 0.15582966],\n",
       "        [ 0.14419209]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(22, 0.57735026918962584), (33, 0.57735026918962584), (41, 0.57735026918962584)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# https://radimrehurek.com/gensim/\n",
    "import gensim\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(docs).todense()\n",
    "\n",
    "corpus = [list(filter(lambda x: x[1] != 0, enumerate(np.asarray(row)[0]))) for row in x]\n",
    "tfidf = TfidfModel(corpus)\n",
    "print tfidf[corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Байесовский классификатор\n",
    "\n",
    "Дано\n",
    "\n",
    "$\\mathbf{x} \\in X$ - описание документа $d$ из коллекции $D$  \n",
    "$C_k \\in C, \\; k = 1,\\ldots,K$ - целевая переменная\n",
    "\n",
    "Теорема Байеса\n",
    "$$\n",
    "P(C_k \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid C_k) p(C_k)}{p(\\mathbf{x})} \\propto p(\\mathbf{x} \\mid C_k) p(C_k)\n",
    "$$\n",
    "\n",
    "Принцип Maximum A-Posteriori\n",
    "$$\n",
    "C_{MAP} = \\arg \\max_k p(C_k | \\mathbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Байесовский классификатор — широкий класс алгоритмов классификации, основанный на принципе максимума апостериорной вероятности.  \n",
    "Для классифицируемого объекта вычисляются функции правдоподобия каждого из классов, по ним вычисляются апостериорные вероятности классов.  \n",
    "Объект относится к тому классу, для которого апостериорная вероятность максимальна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Naive Bayes\n",
    "\n",
    "$x_j$ - слово на $j$-м месте в документе $\\mathbf{x}$,  \n",
    "$w^i \\in V$ - слово из словаря $V$\n",
    "\n",
    "\n",
    "Предположения\n",
    "* conditional independence - слова внутри документа независимы\n",
    "$$\n",
    "p(x_i=w^s, x_j=w^r | C_k) = p(x_i=w^s | C_k) p(x_j=w^r | C_k)\n",
    "$$\n",
    "* postional independence - результат не зависит от позиции слова в документе\n",
    "$$\n",
    "P(x_i=w^s | C_k) = P(x_j=w^s | C_k) = P(x = w^s | C_k)\n",
    "$$\n",
    "\n",
    "Получаем\n",
    "$$\n",
    "p(\\mathbf{x} | C_k) = p(x_1=w^{s_1}, \\ldots, x_{|\\mathbf{x}|}=w^{s_{|\\mathbf{x}|}} | C_k) = \\prod_{i=1}^{|\\mathbf{x}|} p(x = w^{s_i} | C_k)\n",
    "$$\n",
    "\n",
    "**Почему NB хорошо работает?**  \n",
    "Корректная оценка дает правильное предсказание, но правильное предсказание *не требует* корректной оценки\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Варианты NB\n",
    "\n",
    "MAP\n",
    "$$\n",
    "C_{MAP} = \\arg \\max_k p(C_k) \\prod_{i=1}^{|\\mathbf{x}|} p(x = w^{s_i} | C_k)  = \n",
    "$$\n",
    "$$\n",
    "= \\arg \\max_k \\left[ \\log p(C_k) + \\sum_{i=1}^{|\\mathbf{x}|} \\log p(x = w^{s_i} | C_k) \\right]\n",
    "$$\n",
    "Априорные вероятности\n",
    "$$\n",
    "p(C_k) = N_{C_k}/{N}\n",
    "$$\n",
    "Likelihood $p(x = w^{s_i} | C_k)$\n",
    "* BernoulliNB $p(x = w^{s_i} | C_k) = D_{w^{s_i}, C_k} / D_{C_k}$, $D$ - кол-во документов\n",
    "* MultinomialNB $p(x = w^{s_i} | C_k) = T_{w^{s_i}, C_k} / T_{C_k}$, $T$ - кол-во токенов\n",
    "* GaussianNB $p(x = w^{s_i} | C_k) = \\mathcal{N}(\\mu_k, \\sigma_k^2)$, параметры из MLE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение NB\n",
    "\n",
    "```\n",
    "function nb_train(D,C):\n",
    "\tV = dictionary of tokens\n",
    "\tN = number of documents\n",
    "\tfor Ck in C: # iterate over all classes\n",
    "\t\tN_Ck = number of documents in class Ck\n",
    "\t\tp(Ck) = N_Ck / N # Class prior\n",
    "\t\tD_Ck = Documents in class Ck\t\t\n",
    "\t\tfor w_i in V:\t\t\t\n",
    "\t\t\t# multinomial, bernoulli, gaussian\n",
    "\t\t\tp(w_i|Ck) = count_likelihood(...)\n",
    "\treturn V, p(Ck), p(w_i|Ck)\n",
    "```\n",
    "\n",
    "Алгоритмическая сложность: $O(|D| \\langle |\\mathbf{x}| \\rangle + |C||V|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение MultinomialNB\n",
    "\n",
    "\n",
    "```\n",
    "function nb_apply(d, C, V, p(Ck), p(w_i|Ck)):\n",
    "\tx = tokenize(d) # somehow\t\n",
    "\tfor Ck in C: # iterate over all classes\n",
    "\t\tscore(Ck|x) = log p(Ck) # use class prior\n",
    "\t\t# use likelihoods\n",
    "\t\tfor i in 1..|x|:\t\t\n",
    "\t\t\tscore(Ck|x) += log p(x_i|Ck)\n",
    "\treturn arg max score(Ck|x)\n",
    "```\n",
    "\n",
    "Алгоритмическая сложность: $O(|C||\\mathbf{x}|)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сглаживание\n",
    "\n",
    "Проблема: $p(свинки|мимими) = 0$\n",
    "\n",
    "Решение:\n",
    "\n",
    "$$ p(x=w_{s_i}|C_k) = \\frac{ T_{w^{s_i}, C_k} + \\alpha }{ T_{C_k} + \\alpha|V|} $$\n",
    "\n",
    "\n",
    "если $\\alpha \\geq 0$ - сглаживание Лапласа, если $0 \\leq \\alpha \\leq 1$ - Лидстоуна\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Example No.| Color |Type| Origin|Stolen?|\n",
    "|--|--|--|--|--|\n",
    "|1| Red |Sports |Domestic |Yes|\n",
    "|2| Red |Sports |Domestic |No|\n",
    "|3| Red |Sports |Domestic |Yes|\n",
    "|4| Yellow |Sports |Domestic |No|\n",
    "|5| Yellow |Sports |Imported |Yes|\n",
    "|6| Yellow |SUV |Imported| No|\n",
    "|7| Yellow |SUV |Imported |Yes|\n",
    "|8| Yellow |SUV |Domestic |No|\n",
    "|9| Red |SUV |Imported |No|\n",
    "|10| Red |Sports| Imported |Yes|\n",
    "\n",
    "x = Red Domestic SUV   \n",
    "y = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(Ck):  \n",
    "p(Yes) = 5 / 10  \n",
    "p(No) = 5 / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(Red|Yes) = 3 / 15  \n",
    "P(Red|No) = 2 / 15  \n",
    "p(Domestic|Yes) = 2 / 15  \n",
    "p(Domestic|No) = 3 / 15  \n",
    "p(SUV|Yes) = 1 / 15  \n",
    "p(SUV|No) = 3 / 15  \n",
    "\n",
    "p(Yes|x) = p(Yes) * p(Red|Yes) * p(Domestic|Yes) * p(SUV|Yes) = 5/10 * 3/15 * 2/15 * 1/15  \n",
    "p(No|x) = p(No) * p(Red|No) * p(Domestic|No) * p(SUV|No) = 5/10 * 2/15 * 3/15 * 3/15  \n",
    "\n",
    "No!\n",
    "\n",
    "Проделать то же самое со сглаживанием и предсказать  Black Domestic SUV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**+** (Удивительно) неплохо работает  \n",
    "**+** Стабилен при смещении выборки  \n",
    "**+** Оптимальный по производительности  \n",
    "\n",
    "**-** Наивные предположения  \n",
    "**-** Требует отбора признаков  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam  detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv', usecols=[0, 1], encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "y = pd.get_dummies(df['v1'])['spam']\n",
    "X = vectorizer.fit_transform(df['v2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.value_counts of 0        ham\n",
       "1        ham\n",
       "2       spam\n",
       "3        ham\n",
       "4        ham\n",
       "5       spam\n",
       "6        ham\n",
       "7        ham\n",
       "8       spam\n",
       "9       spam\n",
       "10       ham\n",
       "11      spam\n",
       "12      spam\n",
       "13       ham\n",
       "14       ham\n",
       "15      spam\n",
       "16       ham\n",
       "17       ham\n",
       "18       ham\n",
       "19      spam\n",
       "20       ham\n",
       "21       ham\n",
       "22       ham\n",
       "23       ham\n",
       "24       ham\n",
       "25       ham\n",
       "26       ham\n",
       "27       ham\n",
       "28       ham\n",
       "29       ham\n",
       "        ... \n",
       "5542     ham\n",
       "5543     ham\n",
       "5544     ham\n",
       "5545     ham\n",
       "5546     ham\n",
       "5547    spam\n",
       "5548     ham\n",
       "5549     ham\n",
       "5550     ham\n",
       "5551     ham\n",
       "5552     ham\n",
       "5553     ham\n",
       "5554     ham\n",
       "5555     ham\n",
       "5556     ham\n",
       "5557     ham\n",
       "5558     ham\n",
       "5559     ham\n",
       "5560     ham\n",
       "5561     ham\n",
       "5562     ham\n",
       "5563     ham\n",
       "5564     ham\n",
       "5565     ham\n",
       "5566    spam\n",
       "5567    spam\n",
       "5568     ham\n",
       "5569     ham\n",
       "5570     ham\n",
       "5571     ham\n",
       "Name: v1, Length: 5572, dtype: object>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['v1'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_cv(model, param_grid, x_train, y_train):\n",
    "    grid_search = RandomizedSearchCV(model, param_grid, cv=5, scoring='accuracy', n_iter=10)\n",
    "    t_start = time.time()\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    t_end = time.time()\n",
    "    print('model {} best accuracy score is {}'.format(model.__class__.__name__, grid_search.best_score_))\n",
    "    print('time for training is {} seconds'.format(t_end - t_start))\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model MultinomialNB best accuracy score is 0.981516206804\n",
      "time for training is 0.304975986481 seconds\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha':[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 5]}\n",
    "model = MultinomialNB()\n",
    "best_model = randomized_cv(model, param_grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980967917346\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([u'no', u'just', u'teaches', u'choose', u'funny', u'wife',\n",
       "        u'tsunamis', u'nobody', u'fact', u'hw', u'natural', u'happens',\n",
       "        u'volcanoes', u'erupt', u'arise', u'hurricanes', u'sway',\n",
       "        u'aroundn', u'disasters'],\n",
       "       dtype='<U34'),\n",
       " array([u'to', u'think', u'and', u'you', u'for', u'my', u'is', u'me',\n",
       "        u'had', u'the', u'on', u'cost', u'if', u'are', u'do', u'of', u'her',\n",
       "        u'joke', u'one', u'research', u'contact', u'school', u'sent',\n",
       "        u'also', u'few', u'thinking', u'less', u'schools', u'ones',\n",
       "        u'scores', u'sophas', u'secondary', u'application', u'applying',\n",
       "        u'ogunrinde', u'expensive'],\n",
       "       dtype='<U34'),\n",
       " array([u'to', u'you', u'call', u'150p', u'pobox', u'that', u'we', u'know',\n",
       "        u'out', u'who', u'find', u'someone', u'fancies', u'09058097218',\n",
       "        u'ls15hb'],\n",
       "       dtype='<U34'),\n",
       " array([u'only', u'in', u'ok', u'to', u'text', u'it', u'and', u'you', u'me',\n",
       "        u'as', u'your', u'the', u'soon', u'promise', u'if', u'can',\n",
       "        u'getting', u'll', u'let', u'know', u'out', u'morning', u'made'],\n",
       "       dtype='<U34'),\n",
       " array([u'free', u'entry', u'to', u'txt', u'100', u'our', u'www', u'ur',\n",
       "        u'of', u'awarded', u'draw', u'500', u'congratulations', u'cd',\n",
       "        u'vouchers', u'music', u'87066', u'tncs', u'ldew',\n",
       "        u'com1win150ppmx3age16', u'weekly', u'either', u'gift'],\n",
       "       dtype='<U34'),\n",
       " array([u'text', u'and', u'you', u'on', u'll', u'let', u'know', u'carlos',\n",
       "        u'hang'],\n",
       "       dtype='<U34'),\n",
       " array([u'now', u'you', u'did', u'are', u'see', u'where'],\n",
       "       dtype='<U34'),\n",
       " array([u'no', u'message', u'what', u'responce', u'happend'],\n",
       "       dtype='<U34'),\n",
       " array([u'in', u'to', u'and', u'right', u'at', u'first', u'down', u'turn',\n",
       "        u'lt', u'gt', u'get', u'cut', u'gandhipuram', u'walk', u'cross',\n",
       "        u'road', u'side', u'street'],\n",
       "       dtype='<U34'),\n",
       " array([u'you', u'your', u'yet', u'shit', u'flippin'],\n",
       "       dtype='<U34')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(X_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://code.google.com/archive/p/word2vec/#Pre-trained_word_and_phrase_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'freebase-vectors-skipgram1000-en.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-42106550b8fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"freebase-vectors-skipgram1000-en.bin.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vacation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/smart_open/smart_open_lib.pyc\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3u'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/smart_open/smart_open_lib.pyc\u001b[0m in \u001b[0;36mfile_smart_open\u001b[0;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0mraw_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'freebase-vectors-skipgram1000-en.bin.gz'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "fn = \"freebase-vectors-skipgram1000-en.bin.gz\"\n",
    "model = KeyedVectors.load_word2vec_format(fn)\n",
    "model.most_similar('vacation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dumps.wikimedia.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "wiki = WikiCorpus('ruwiki-20171220-pages-articles-multistream.xml.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser, Phrases\n",
    "bigram = Phrases(wiki.get_texts())\n",
    "bigram_transformer = Phraser(bigram)\n",
    "\n",
    "\n",
    "def text_generator_bigram():\n",
    "    for text in wiki.get_texts():\n",
    "        yield bigram_transformer[[word.decode('utf8') for word in text]]\n",
    "        \n",
    "        \n",
    "def text_generator_trigram():\n",
    "    for text in wiki.get_texts():\n",
    "        yield trigram_transformer[bigram_transformer[[word.decode('utf8') for word in text]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(size=100, window=7, min_count=10, workers=10)\n",
    "model.build_vocab(text_generator_trigram())\n",
    "model.train(text_generator_trigram())\n",
    "\n",
    "fname = 'w2v_model_wiki'\n",
    "model.save(fname)\n",
    "model = Word2Vec.load(fname)\n",
    "model.most_similar('токен')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста для Java:  \n",
    "https://stanfordnlp.github.io/CoreNLP/index.html  \n",
    "https://opennlp.apache.org/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Free', u'PERSON')\n",
      "(u'2', u'CARDINAL')\n",
      "(u'FA Cup', u'EVENT')\n",
      "(u'21st', u'ORDINAL')\n",
      "(u'May 2005', u'DATE')\n",
      "(u'87121', u'DATE')\n",
      "(u'rate)T&C', u'ORG')\n",
      "(u'08452810075over18', u'PERSON')\n",
      "(u'Nah', u'PERSON')\n",
      "(u\"3 week's\", u'DATE')\n",
      "(u'Melle Melle', u'PERSON')\n",
      "(u'Oru Minnaminunginte Nurungu Vettam', u'PERSON')\n",
      "(u'9', u'CARDINAL')\n",
      "(u'KL341', u'CARDINAL')\n",
      "(u'Valid', u'FAC')\n",
      "(u'12 hours', u'TIME')\n",
      "(u'11 months', u'DATE')\n",
      "(u'U R', u'ORG')\n",
      "(u'Update', u'GPE')\n",
      "(u'The Mobile Update Co FREE', u'ORG')\n",
      "(u'08002986030', u'DATE')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Process whole documents\n",
    "\n",
    "text = '. '.join(df['v2'][:10])\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat.... Ok lar... Joking wif u oni.... Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's. U dun say so early hor... U c already then say.... Nah I don't think he goes to usf, he lives around here though. FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, \\xe5\\xa31.50 to rcv. Even my brother is not like to speak with me. They treat me like aids patent.. As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune. WINNER!! As a valued network customer you have been selected to receivea \\xe5\\xa3900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.. Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42931570970586602"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine semantic similarities\n",
    "doc1 = nlp(u'the fries were gross')\n",
    "doc2 = nlp(u'worst potato ever')\n",
    "doc1.similarity(doc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86696608592805557"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine semantic similarities\n",
    "doc1 = nlp(u'men')\n",
    "doc2 = nlp(u'women')\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Top 10 most similar words to men:\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "parser = spacy.load('en')\n",
    "nasa = parser.vocab[u'men']\n",
    "\n",
    "# cosine similarity\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"men\"})\n",
    "print(len(allWords))\n",
    "# sort by similarity to NASA\n",
    "allWords.sort(key=lambda w: cosine(w.vector, nasa.vector))\n",
    "allWords.reverse()\n",
    "print(\"Top 10 most similar words to men:\")\n",
    "for word in allWords[:10]:\n",
    "    print(word.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print parser.vocab[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
