{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дерево решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать алгоритм дерева решений для бинарной классификации:\n",
    "\n",
    "* заполнить отсутсвующие методы и реализовать простой вариант алгоритма с перебором всех признаков, всех вариантов разбиений\n",
    "* Применить сортировку по признаку и делать расчеты только при изменении целевой переменной.\n",
    "* реализовать и описать(!) список модификаций, которые могли бы ускорить работу дерева\n",
    "\n",
    "В представленном ниже коде реализованы методы для выполнения предсказания вероятности и классов и для разделения выборки. По ним можно получить подсказки о возможном варианте хранения внутренней структуры дерева. Методы можно использовать, можно полностью изменить.\n",
    "\n",
    "Проверку проделать на датасете Iris.\n",
    "\n",
    "Ниже есть блоки кода для сравнения корректности и скорости работы с реализацией из sklearn.\n",
    "\n",
    "Применить реализованное дерево решений для задачи Titanic на kaggle. Применить для той же задачи дерево решений из sklearn. Применить кросс-валидацию для подбора параметров. Сравнить с результатами предыдущих моделей (на кросс-валидации!). Если результат улучшился - сделать сабмит. Написать отчет о результатах.\n",
    "\n",
    "Дополнительное задание (\\*):\n",
    "* реализовать разные критерии останова\n",
    "* реализовать суррогатный сплит\n",
    "* реализовать разбиение на любое количество узлов (параметр)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    NON_LEAF_TYPE = 0\n",
    "    LEAF_TYPE = 1\n",
    "\n",
    "    def __init__(self,\n",
    "                 min_samples_split=2,\n",
    "                 max_depth=None,\n",
    "                 sufficient_share=1.0,\n",
    "                 criterion='gini',\n",
    "                 max_features=None):\n",
    "        self.tree = dict()\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.sufficient_share = sufficient_share\n",
    "        self.num_class = -1\n",
    "        if criterion == 'gini':\n",
    "            self.G_function = self.__gini\n",
    "        elif criterion == 'entropy':\n",
    "            self.G_function = self.__entropy\n",
    "        elif criterion == 'misclass':\n",
    "            self.G_function = self.__misclass\n",
    "        else:\n",
    "            raise ValueError('invalid criterion name')\n",
    "\n",
    "        if max_features == 'sqrt':\n",
    "            self.get_feature_ids = self.__get_feature_ids_sqrt\n",
    "        elif max_features == 'log2':\n",
    "            self.get_feature_ids = self.__get_feature_ids_log2\n",
    "        elif max_features == None:\n",
    "            self.get_feature_ids = self.__get_feature_ids_N\n",
    "        else:\n",
    "            raise ValueError('invalid max_features name')\n",
    "\n",
    "    def __gini(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = l_s.astype('float')\n",
    "        r_s = r_s.astype('float')\n",
    "        # 1. Мера неопределенности родительского узла\n",
    "        i_p = self.__gini_p((l_c + r_c) / (l_s + r_s))\n",
    "        # 2. Мера неопределенности левого дочернего узла\n",
    "        i_l = self.__gini_p(l_c / l_s)\n",
    "        # 3. Мера неопределенности правого дочернего узла\n",
    "        i_r = self.__gini_p(r_c / r_s)\n",
    "        return self.__impurity(i_p, i_l, i_r, l_s, r_s)\n",
    "    \n",
    "    def __entropy(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = l_s.astype('float')\n",
    "        r_s = r_s.astype('float')\n",
    "        # 1. Мера неопределенности родительского узла\n",
    "        i_p = self.__entropy_p((l_c + r_c) / (l_s + r_s))\n",
    "        # 2. Мера неопределенности левого дочернего узла\n",
    "        i_l = self.__entropy_p(l_c / l_s)\n",
    "        # 3. Мера неопределенности правого дочернего узла\n",
    "        i_r = self.__entropy_p(r_c / r_s)\n",
    "        return self.__impurity(i_p, i_l, i_r, l_s, r_s)\n",
    "\n",
    "    def __misclass(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = l_s.astype('float')\n",
    "        r_s = r_s.astype('float')\n",
    "        # 1. Мера неопределенности родительского узла\n",
    "        i_p = self.__misclass_p((l_c + r_c) / (l_s + r_s))\n",
    "        # 2. Мера неопределенности левого дочернего узла\n",
    "        i_l = self.__misclass_p(l_c / l_s)\n",
    "        # 3. Мера неопределенности правого дочернего узла\n",
    "        i_r = self.__misclass_p(r_c / r_s)\n",
    "        return self.__impurity(i_p, i_l, i_r, l_s, r_s)\n",
    "\n",
    "    def __get_feature_ids_sqrt(self, n_feature):\n",
    "        feature_ids = range(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        max_features = max(1, int(np.sqrt(n_feature)))\n",
    "        return feature_ids[:max_features]\n",
    "        \n",
    "    def __get_feature_ids_log2(self, n_feature):\n",
    "        feature_ids = range(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        max_features = max(1, int(np.log2(n_feature)))\n",
    "        return feature_ids[:max_features]\n",
    "\n",
    "    def __get_feature_ids_N(self, n_feature):\n",
    "        return range(n_feature)\n",
    "    \n",
    "    def __sort_samples(self, x, y):\n",
    "        sorted_idx = x.argsort()\n",
    "        return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "    def __div_samples(self, x, y, feature_id, threshold):\n",
    "        left_mask = x[:, feature_id] > threshold\n",
    "        right_mask = ~left_mask\n",
    "        return x[left_mask], x[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def __find_threshold(self, x, y):\n",
    "        # Сортируем значения свойства по возрастанию и приводим к аналогичной последовательности целевые переменные.\n",
    "        sorted_x, sorted_y = self.__sort_samples(x, y)\n",
    "        # Количество меток класса.\n",
    "        class_number = np.unique(y).shape[0]\n",
    "\n",
    "        # Здесь, по всей видимости, мы обрезаем образцы с учетом параметра min_samples_split, для того, \n",
    "        # чтобы в середине осталось необходимое количество образцов размера min_samples_split. \n",
    "        # Хотя это немного странный подход, можно проверять min_samples_split заранее.\n",
    "        # И еще интересный момент, если min_samples_split = 2, то такой код отрежет 4\n",
    "        # элемента, т.е. остановит расщепление заранее если в узле 4 или 5 элементов.\n",
    "        # Непонятно баг это или фича.\n",
    "        splitted_sorted_y = sorted_y[self.min_samples_split:-self.min_samples_split]\n",
    "        # Сравниваем каждый элемент с предыдущим, таким образом мы получаем информацию о том насколько неоднородная выборка,\n",
    "        # это будет иметь влияния на вычисление меры неоднородности.\n",
    "        r_border_ids = np.where(splitted_sorted_y[:-1] != splitted_sorted_y[1:])[0] + (self.min_samples_split + 1)\n",
    "        \n",
    "        # Если выборка однородная, то нет необходимости делать расщепление.\n",
    "        if len(r_border_ids) == 0:\n",
    "            return float('+inf'), None\n",
    "        \n",
    "        # Что происходит деталях объяснить не так легко, но судя по отладочной информации, \n",
    "        # основной смысл данного участка кода - сформировать различные варианты разбиения классов, \n",
    "        # на основе которых можно вычислить различные меры неоднородности и выбрать из них наиболее оптимальную.\n",
    "        eq_el_count = r_border_ids - np.append([self.min_samples_split], r_border_ids[:-1])\n",
    "        one_hot_code = np.zeros((r_border_ids.shape[0], class_number))\n",
    "        one_hot_code[np.arange(r_border_ids.shape[0]), sorted_y[r_border_ids - 1]] = 1\n",
    "        class_increments = one_hot_code * eq_el_count.reshape(-1, 1)\n",
    "        class_increments[0] = class_increments[0] \\\n",
    "            + np.bincount(sorted_y[:self.min_samples_split], minlength=class_number)\n",
    "\n",
    "        # Здесь создаются различные варианты разбиения классов по узлам дерева:\n",
    "        # l_class_count - варианты разбиения для левого узла\n",
    "        # r_class_count - варианты разбиения для правого узла\n",
    "        # l_sizes - общее число образцов в левом узле\n",
    "        # r_sizes - общее число образцов в правом узле\n",
    "        l_class_count = np.cumsum(class_increments, axis=0)        \n",
    "        r_class_count = np.bincount(y) - l_class_count\n",
    "        l_sizes = r_border_ids.reshape(l_class_count.shape[0], 1)\n",
    "        r_sizes = sorted_y.shape[0] - l_sizes\n",
    "\n",
    "        # Вычисляем возможные меры неоднородности для всех вариантов разбиения классов.\n",
    "        # Выбираем индекс наименьшей из них, как наиболее оптимальный. \n",
    "        gs = self.G_function(l_class_count, l_sizes, r_class_count, r_sizes)\n",
    "        idx = np.argmin(gs)\n",
    "    \n",
    "        # По индексу лучшей неоднородности, определяем количество элементов в левом узле.\n",
    "        # Возвращаем значение лучшей меры неоднородности и среднее значение элемента выборки \n",
    "        # (сам элемент и его сосед слева), который и будет являться тем самым параметром threshold, \n",
    "        # по которому будет определяться разделение выборки на правый и левый узлы.\n",
    "        left_el_id = l_sizes[idx][0]\n",
    "        return gs[idx], (sorted_x[left_el_id-1] + sorted_x[left_el_id]) / 2.0\n",
    "\n",
    "    def __fit_node(self, x, y, node_id, depth, pred_f=-1):\n",
    "        # Если узел содержит только образцы одного класса, то останавливаем расщепление.\n",
    "        if np.unique(y).shape[0] == 1:\n",
    "            self.tree[node_id] = (\n",
    "                self.LEAF_TYPE,\n",
    "                y[0],\n",
    "                0,\n",
    "                0,\n",
    "                y,\n",
    "                'Node contains only samples of one class',\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Если доля образцов одного класса больше или равна необходимой доли для расщепления, то останавливаем расщепление.\n",
    "        most_common_y = Counter(y).most_common(1).pop()\n",
    "        if most_common_y[1] >= self.sufficient_share * y.shape[0]:\n",
    "            self.tree[node_id] = (\n",
    "                self.LEAF_TYPE,\n",
    "                most_common_y[0],\n",
    "                0,\n",
    "                0,\n",
    "                y,\n",
    "                'Samples of one class is greater than or equal sufficient_share',\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Если достигнута максимальная глубина дерева, то останавливаем расщепление.\n",
    "        # Делаем предсказание по классу с наибольшим количеством образцов.\n",
    "        if self.max_depth == depth:\n",
    "            self.tree[node_id] = (\n",
    "                self.LEAF_TYPE,\n",
    "                most_common_y[0],\n",
    "                0,\n",
    "                0,\n",
    "                y,\n",
    "                'Maximum depth of the tree',\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Если количество образцов меньше требуемого для разделения узла, то останавливаем расщепление.\n",
    "        # Делаем предсказание по классу с наибольшим количеством образцов.\n",
    "        if y.shape[0] < self.min_samples_split:\n",
    "            self.tree[node_id] = (\n",
    "                self.LEAF_TYPE,\n",
    "                most_common_y[0],\n",
    "                0,\n",
    "                0,\n",
    "                y,\n",
    "                'Number of samples is less than min_samples_split',\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        n_samples, n_features = x.shape\n",
    "        split_data = []\n",
    "        # Ищем признак по которому будем делать разбиение (который ведет к самому большому приросту информации).\n",
    "        # Для этого необходимо вычислить для каждого признака меру неоднородности (impurity).\n",
    "        # Чем ниже неоднородность, тем выше прирост информации.\n",
    "        # Также здесь мы вычисляем порог (threshold), \n",
    "        # по которому будем определять идти в правый узел дерева или левый (т.к. дерево у нас бинарное).\n",
    "        for feature_id in self.get_feature_ids(n_features):\n",
    "            impurity, threshold = self.__find_threshold(x[:,feature_id], y)\n",
    "            if threshold is not None:\n",
    "                split_data.append((impurity, threshold, feature_id,))\n",
    "\n",
    "        # Если недостаточно данных для разбиения, например, \n",
    "        # достигнут предел минимального количества образцов, то останавливаем расщепление.\n",
    "        # Делаем предсказание по классу с наибольшим количеством образцов.\n",
    "        if not split_data:\n",
    "            self.tree[node_id] = (\n",
    "                self.LEAF_TYPE,\n",
    "                most_common_y[0],\n",
    "                0,\n",
    "                0,\n",
    "                y,\n",
    "                'Empty split data',\n",
    "            )\n",
    "            return\n",
    "\n",
    "        best_split = min(split_data, key=lambda x: x[0])\n",
    "            \n",
    "        x_left, x_right, y_left, y_right = self.__div_samples(x, y, best_split[2], best_split[1])\n",
    "\n",
    "        # Если после расщепления, какой либо узел оказался пустой, то значит, данный узел расщепить невозможно, \n",
    "        # поэтому он будет являться листом.\n",
    "        # Делаем предсказание по классу с наибольшим количеством образцов.\n",
    "        if y_left.shape[0] == 0 or y_right.shape[0] == 0:\n",
    "            self.tree[node_id] = (\n",
    "                self.LEAF_TYPE,\n",
    "                most_common_y[0],\n",
    "                0,\n",
    "                0,\n",
    "                y,\n",
    "                'After splitting one of the nodes is empty',\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            self.tree[node_id] = (self.NON_LEAF_TYPE, best_split[2], best_split[1], best_split[0], y, None)\n",
    "            self.__fit_node(x_left, y_left, 2 * node_id + 1, depth + 1)\n",
    "            self.__fit_node(x_right, y_right, 2 * node_id + 2, depth + 1)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.num_class = np.unique(y).size\n",
    "        self.__fit_node(x, y, 0, 0) \n",
    "\n",
    "    def __predict_class(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold, *_ = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_class(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_class(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[1]\n",
    "\n",
    "    def __predict_probs(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold, *_ = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_probs(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_probs(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            # Изначально было node[2], похоже опечатка...\n",
    "            return node[1]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.array([self.__predict_class(x, 0) for x in X])\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        return np.array([self.__predict_probs(x, 0) for x in X])\n",
    "\n",
    "    def fit_predict(self, x_train, y_train, predicted_x):\n",
    "        self.fit(x_train, y_train)\n",
    "        return self.predict(predicted_x)\n",
    "\n",
    "    @staticmethod\n",
    "    def __gini_p(p):\n",
    "        return 1 - (p ** 2).sum(axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def __entropy_p(p):\n",
    "        return - np.nansum(p * np.log2(p), axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def __misclass_p(p):   \n",
    "        return 1 - p.max(axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def __impurity(i_p, i_l, i_r, l_s, r_s):\n",
    "        t_s = l_s + r_s\n",
    "        return i_p - (np.squeeze(np.asarray(l_s / t_s)) * i_l) - (np.squeeze(np.asarray(r_s / t_s)) * i_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data # petal length and width\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0042760372161865234\n",
      "0.0007078647613525391\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "my_clf.fit(X, y)\n",
    "t2 = time()\n",
    "print(t2 - t1)\n",
    "\n",
    "t1 = time()\n",
    "clf.fit(X, y)\n",
    "t2 = time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.75\n",
      "0.85\n",
      "0.95\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "for train, test in gkf.split(X, y):\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    my_clf.fit(X_train, y_train)\n",
    "    print(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "1.0\n",
      "0.95\n",
      "0.95\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "for train, test in gkf.split(X, y):\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирую Новый Алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "# Попробую пока что разобратся на бинарной классификации\n",
    "X = iris.data # petal length and width\n",
    "target = iris.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итак посчитаем Энтропию на игрушечном примере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [1,1,1,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Нам нужно получить разбиение нашей выборки по значению\n",
    "splits = split(X[:, 2], target['y'], 4)\n",
    "splits\n",
    "# Предположим мы попали в такое разбиение! Они все будут перебиратся если что"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Энтропия всей выборки\n",
    "f_entropy(target['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4718902859406985"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Энтропия левой части\n",
    "f_entropy(splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Энтропия правой части\n",
    "f_entropy(splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4052941061361192"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain(target['y'],splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_entropy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = split(X[:, 2], target['y'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4052941061361192"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain = information_gain(target['y'], splits)\n",
    "gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_entropy(p):\n",
    "    # Convert values to probability\n",
    "    p = np.bincount(p) / float(p.shape[0])\n",
    "\n",
    "    ep = stats.entropy(p)\n",
    "    if ep == -float('inf'):\n",
    "        return 0.0\n",
    "    return ep\n",
    "\n",
    "\n",
    "def information_gain(y, splits):\n",
    "    splits_entropy = sum([f_entropy(split) * (float(split.shape[0]) / y.shape[0]) for split in splits])\n",
    "    return f_entropy(y) - splits_entropy\n",
    "\n",
    "\n",
    "def split(X, y, value):\n",
    "    left_mask = (X < value)\n",
    "    right_mask = (X >= value)\n",
    "    return y[left_mask], y[right_mask]\n",
    "\n",
    "def find_splits(X):\n",
    "        \"\"\"Find all possible split values.\"\"\"\n",
    "        split_values = set()\n",
    "\n",
    "        # Отбираем уникальные значения переменной\n",
    "        x_unique = list(np.unique(X))\n",
    "        \n",
    "        for i in range(1, len(x_unique)):\n",
    "            \n",
    "            # Отбираем промежутки между этими переменныеми\n",
    "            average = (x_unique[i - 1] + x_unique[i]) / 2.0\n",
    "            split_values.add(average)\n",
    "\n",
    "        return list(split_values)\n",
    "\n",
    "def find_best_split(X, target, n_features):\n",
    "        \"\"\"Find best feature and value for a split. Greedy algorithm.\"\"\"\n",
    "\n",
    "        # Sample random subset of features\n",
    "        # subset = random.sample(list(range(0, X.shape[1])), n_features)\n",
    "        max_gain, max_col, max_val = None, None, None\n",
    "\n",
    "        for column in subset:\n",
    "            \n",
    "            # Здесь мы ищем уникальные значения и возвращаем промежутки между ними\n",
    "            split_values = find_splits(X[:, column])\n",
    "            \n",
    "            for value in split_values:\n",
    "                    # Разделяем выборку по признаку и значению/ тоже самое и с целевой меткой\n",
    "                    splits = split(X[:, column], target, value)\n",
    "                    \n",
    "                    # Считаем Прирост информации для этого разбиения\n",
    "                    gain = information_gain(target, splits)\n",
    "                \n",
    "                    # условие останова по приросту информации\n",
    "                    # Если новый gain больше предыдущего то мы продолжаем! искать, если нет то останаваливаемся\n",
    "                    \n",
    "                    if (max_gain is None) or (gain > max_gain):\n",
    "                        \n",
    "                        max_col, max_val, max_gain = column, value, gain\n",
    "                        \n",
    "                        \n",
    "               # Возвращаем колонку значение и прирост информации     \n",
    "        return max_col, max_val, max_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2.45, 0.6931471805599453)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_split(X,target,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Хорошо Я могу найти лучшее разбиение! Что дальше? Нужно сделать этот процесс рекурсивным и прописать правила образования листьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коека рабочий вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    \"\"\"Recursive implementation of decision tree.\"\"\"\n",
    "\n",
    "    def __init__(self,max_features=None, min_samples_split=2, max_depth=None):\n",
    "        \n",
    "        self.impurity = None\n",
    "        self.threshold = None\n",
    "        self.column_index = None\n",
    "        self.outcome = None\n",
    "        self.max_features = max_features\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        \n",
    "        \n",
    "         \n",
    "       \n",
    "        \n",
    "    # Проверяем узел это или лист   \n",
    "        \n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return not bool(self.left_child and self.right_child)    \n",
    "        \n",
    "        # Считаем энтропию\n",
    "\n",
    "    def f_entropy(self,p):\n",
    "        \n",
    "        p = np.bincount(p) / float(p.shape[0])\n",
    "\n",
    "        ep = stats.entropy(p)\n",
    "        if ep == -float('inf'):\n",
    "            return 0.0\n",
    "        return ep\n",
    "\n",
    "        # Выводим Прирост информации\n",
    "        \n",
    "    def information_gain(self, y, splits):\n",
    "        splits_entropy = sum([self.f_entropy(split) * (float(split.shape[0]) / y.shape[0]) for split in splits])\n",
    "        return self.f_entropy(y) - splits_entropy\n",
    "\n",
    "        # Делаем сплит по  значению\n",
    "        \n",
    "    def split(self, X, y, value):\n",
    "        left_mask = (X < value)\n",
    "        right_mask = (X >= value)\n",
    "        return y[left_mask], y[right_mask]\n",
    "    \n",
    "    \n",
    "    def get_split_mask(self,X, column, value):\n",
    "        left_mask = (X[:, column] < value)\n",
    "        right_mask = (X[:, column] >= value)\n",
    "        return left_mask, right_mask\n",
    "    \n",
    "    \n",
    "    def split_dataset(self,X, target, column, value, return_X=True):\n",
    "        \n",
    "        left_mask, right_mask = self.get_split_mask(X, column, value)\n",
    "\n",
    "        left, right = [], []\n",
    "        \n",
    "        left = target[left_mask]\n",
    "        right = target[right_mask]\n",
    "\n",
    "        if return_X:\n",
    "            left_X, right_X = X[left_mask], X[right_mask]\n",
    "            return left_X, right_X, left, right\n",
    "        else:\n",
    "            return left, right\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Отбираем все промежуточные значения где можно сделать сплит\n",
    "    def find_splits(self, X):\n",
    "        \"\"\"Find all possible split values.\"\"\"\n",
    "        split_values = set()\n",
    "        x_unique = list(np.unique(X))\n",
    "        \n",
    "        for i in range(1, len(x_unique)):\n",
    "            average = (x_unique[i - 1] + x_unique[i]) / 2.0\n",
    "            split_values.add(average)\n",
    "\n",
    "        return list(split_values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        # Отбираем лучшее разбиение по приросту информации(Чем ближе к энтропии таргета тем лучше)\n",
    "        \n",
    "    def find_best_split(self,X, target, n_features):\n",
    "        \"\"\"Find best feature and value for a split. Greedy algorithm.\"\"\"\n",
    "\n",
    "        # Случайные признаки\n",
    "        subset = random.sample(list(range(0, X.shape[1])), n_features)\n",
    "        max_gain, max_col, max_val = None, None, None\n",
    "\n",
    "        for column in subset:\n",
    "            \n",
    "            # Здесь мы ищем уникальные значения и возвращаем промежутки между ними\n",
    "            split_values = self.find_splits(X[:, column])\n",
    "            \n",
    "            for value in split_values:\n",
    "                    # Разделяем выборку по признаку и значению/ тоже самое и с целевой меткой\n",
    "                    splits = self.split(X[:, column], target, value)\n",
    "                    \n",
    "                    # Считаем Прирост информации для этого разбиения\n",
    "                    gain = self.information_gain(target, splits)\n",
    "                \n",
    "                    # условие останова по приросту информации\n",
    "                    # Если новый gain больше предыдущего то мы продолжаем! искать, если нет то останаваливаемся\n",
    "                    \n",
    "                    if (max_gain is None) or (gain > max_gain):\n",
    "                        \n",
    "                        max_col, max_val, max_gain = column, value, gain\n",
    "                        \n",
    "                        \n",
    "               # Возвращаем колонку значение и прирост информации     \n",
    "        return max_col, max_val, max_gain\n",
    "\n",
    "    def train(self,X, target, max_features=None, min_samples_split=2, max_depth=None):\n",
    "        \"\"\"Build a decision tree from training set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "            Feature dataset.\n",
    "        target : dictionary or array-like\n",
    "            Target values.\n",
    "        max_features : int or None\n",
    "            The number of features to consider when looking for the best split.\n",
    "        min_samples_split : int\n",
    "            The minimum number of samples required to split an internal node.\n",
    "        max_depth : int\n",
    "            Maximum depth of the tree.\n",
    "        minimum_gain : float, default 0.01\n",
    "            Minimum gain required for splitting.\n",
    "        loss : function, default None\n",
    "            Loss function for gradient boosting.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "        # Здесь мы прописываем условия развития дерева\n",
    "        \n",
    "        try:\n",
    "            # Заканчиваем если Кол-во объектов, Максимальная глубина\n",
    "            assert (X.shape[0] > min_samples_split)\n",
    "            assert (max_depth > 0)\n",
    "\n",
    "            if max_features is None:\n",
    "                max_features = X.shape[1]\n",
    "                \n",
    "            column, value, gain = self.find_best_split(X, target, max_features)\n",
    "            \n",
    "            \n",
    "            assert gain is not None\n",
    "        \n",
    "\n",
    "            self.column_index = column\n",
    "            self.threshold = value\n",
    "            self.impurity = gain\n",
    "\n",
    "            # Split dataset\n",
    "            left_X, right_X, left_target, right_target = self.split_dataset(X, target, column, value)\n",
    "\n",
    "            # Grow left and right child\n",
    "            self.left_child = Tree()\n",
    "            self.left_child.train(left_X, left_target, max_features, min_samples_split, max_depth - 1)\n",
    "\n",
    "            self.right_child = Tree()\n",
    "            self.right_child.train(right_X, right_target, max_features, min_samples_split, max_depth - 1)\n",
    "            \n",
    "        except AssertionError:\n",
    "            self._calculate_leaf_value(target)\n",
    "\n",
    "    def _calculate_leaf_value(self, target):\n",
    "        \"\"\"Find optimal value for leaf.\"\"\"\n",
    "        \n",
    "        # Задаем вероятность отнесения к классу в узле\n",
    "        # self.outcome = stats.itemfreq(target)[:, 1] / float(target.shape[0])\n",
    "        \n",
    "        # Определяем Максимально встречающийся класс\n",
    "        \n",
    "        self.outcome = np.max(target)\n",
    "        \n",
    "\n",
    "    def predict_row(self, row):\n",
    "        \"\"\"Predict single row.\"\"\"\n",
    "#         Если узел то движемся дальше\n",
    "        if not self.is_terminal:\n",
    "            if row[self.column_index] < self.threshold:\n",
    "                return self.left_child.predict_row(row)\n",
    "            else:\n",
    "                return self.right_child.predict_row(row)\n",
    "            \n",
    "#             Если лист то возвращаем максимальный класс\n",
    "        \n",
    "        return self.outcome\n",
    "\n",
    "    def predict(self, X):\n",
    "        result = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            result[i] = self.predict_row(X[i, :])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Tree = Tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3, stratify=y, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.45 0.6365141682948128\n",
      "1 2.5999999999999996 0.0\n",
      "0 --------ВозвратКласса\n",
      "0 5.75 0.0\n",
      "2 1.45 0.0\n",
      "0 --------ВозвратКласса\n",
      "0 --------ВозвратКласса\n",
      "0 --------ВозвратКласса\n",
      "2 4.75 0.581997376922989\n",
      "1 2.25 0.0\n",
      "1 --------ВозвратКласса\n",
      "3 1.45 0.0\n",
      "1 --------ВозвратКласса\n",
      "1 --------ВозвратКласса\n",
      "3 1.7000000000000002 0.10706489850859616\n",
      "2 4.95 0.21951214867965618\n",
      "1 --------ВозвратКласса\n",
      "2 --------ВозвратКласса\n",
      "3 1.85 0.0\n",
      "2 --------ВозвратКласса\n",
      "2 --------ВозвратКласса\n"
     ]
    }
   ],
   "source": [
    "Test_Tree.train(x_train,y_train,max_depth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titanic_tree = Tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5 0.150592892745533\n",
      "1 1.5 0.1446051606241726\n",
      "6 -0.07616583290803644 0.028171407647627433\n",
      "3 0.5 0.046687354242364865\n",
      "1 --------ВозвратКласса\n",
      "1 --------ВозвратКласса\n",
      "5 -2.126150089122424 0.07081055490819625\n",
      "0 --------ВозвратКласса\n",
      "1 --------ВозвратКласса\n",
      "6 -0.1641004431432998 0.10710026784250681\n",
      "6 -0.48832777782140896 0.04206281333529227\n",
      "1 --------ВозвратКласса\n",
      "1 --------ВозвратКласса\n",
      "6 -0.1278469208162656 0.0\n",
      "0 --------ВозвратКласса\n",
      "0 --------ВозвратКласса\n",
      "1 0.5 0.033320352237823536\n",
      "5 1.7951836700987749 0.05179265657323706\n",
      "6 -0.13828020116168832 0.05428601284374357\n",
      "0 --------ВозвратКласса\n",
      "1 --------ВозвратКласса\n",
      "2 0.5 0.15829761474955373\n",
      "0 --------ВозвратКласса\n",
      "1 --------ВозвратКласса\n",
      "5 -1.3108232678982144 0.028437569492038628\n",
      "2 2.5 0.3968526217659485\n",
      "1 --------ВозвратКласса\n",
      "1 --------ВозвратКласса\n",
      "2 1.5 0.005045947338773338\n",
      "1 --------ВозвратКласса\n",
      "0 --------ВозвратКласса\n"
     ]
    }
   ],
   "source": [
    "Titanic_tree.train(x_train_full,y_train_full.values,max_depth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11177492141723633\n",
      "0.0007841587066650391\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "Test_Tree.fit(X, y)\n",
    "t2 = time()\n",
    "print(t2 - t1)\n",
    "\n",
    "t1 = time()\n",
    "clf.fit(X, y)\n",
    "t2 = time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скорость отстой!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n",
      "0.9666666666666667\n",
      "0.9\n",
      "1.0\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "gkf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train, test in gkf.split(X, y):\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]   \n",
    "    Test_Tree.fit(X_train, y_train)\n",
    "    print(accuracy_score(y_pred=Test_Tree.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Качество на уровне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n",
      "0.9666666666666667\n",
      "0.9\n",
      "0.9\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "gkf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train, test in gkf.split(X, y):\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]   \n",
    "    clf.fit(X_train, y_train)\n",
    "    print(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('/Users/imac/Desktop/Python/Otus/Lesson8_Future_Eng_Titanick/train.csv', index_col = 'PassengerId')\n",
    "df_test = pd.read_csv('/Users/imac/Desktop/Python/Otus/Lesson8_Future_Eng_Titanick/test.csv', index_col = 'PassengerId')\n",
    "\n",
    "# Объеденим 2 фрейма чтобы можно было преобразовывать спокойно, но запомним индексы\n",
    "\n",
    "idx_train = df_train.shape\n",
    "\n",
    "idx_split = df_train.shape[0]\n",
    "idx_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сразу выделяю целевой класс\n",
    "Y_TRAIN = df_train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, Imputer\n",
    "from sklearn.preprocessing import CategoricalEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class Simple_pipeline(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        # Embarked содержит 2 пропуска! Я решил заполнить их самым частым признаком\n",
    "        # Возможно так делать не корректно но я хотел добавить именно строковый элемент,\n",
    "        # чтобы можно было преобразовать сразу CategoricalEncoder\n",
    "\n",
    "    def embarked_fill(self, df):\n",
    "        max_count_embarked = df['Embarked'].value_counts().index[0]\n",
    "        df['Embarked'] = df['Embarked'].fillna(max_count_embarked)\n",
    "        return df['Embarked'].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "    def get_sex_pclass_col(self, df):\n",
    "        # Pclass,SibSp,Parch уже числовые поэтому CategoricalEncoder для них сделает просто onehot\n",
    "        return df[['Sex','Pclass','SibSp','Parch']]\n",
    "\n",
    "    def get_num_cols(self, df):\n",
    "        return df[['Age', 'Fare']]\n",
    "    \n",
    "    def get_pipline(self):\n",
    "\n",
    "        pipeline = make_union(*[\n",
    "            make_pipeline(FunctionTransformer(self.get_sex_pclass_col, validate=False), \n",
    "                          CategoricalEncoder(encoding='ordinal')),\n",
    "            \n",
    "            make_pipeline(FunctionTransformer(self.embarked_fill, validate=False),\n",
    "                          CategoricalEncoder(encoding='ordinal')), \n",
    "\n",
    "            make_pipeline(FunctionTransformer(self.get_num_cols, validate=False), Imputer(strategy='mean'),StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        return pipeline.fit_transform(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_Data_full = Simple_pipeline(Data).get_pipline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titanic_tree = Tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full, x_valid_full, y_train_full, y_valid_full = train_test_split(X_Data_full[:idx_split], Y_TRAIN,\n",
    "                                                    test_size=0.3, stratify=Y_TRAIN, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titanic_tree.train(x_train_full,y_train_full,max_depth = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7649253731343284\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(Titanic_tree.predict(x_valid_full),y_valid_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train_full,y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7835820895522388\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred=clf.predict(x_valid_full), y_true=y_valid_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применить для задачи Titanic "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
