{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "random.seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    \"\"\"Recursive implementation of decision tree.\"\"\"\n",
    "\n",
    "    def __init__(self,max_features=None, min_samples_split=2, max_depth=None):\n",
    "        \n",
    "        self.impurity = None\n",
    "        self.threshold = None\n",
    "        self.column_index = None\n",
    "        self.outcome = None\n",
    "        self.max_features = max_features\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "         \n",
    "        \n",
    "    # Проверяем узел это или лист   \n",
    "        \n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return not bool(self.left_child and self.right_child)    \n",
    "        \n",
    "        # Считаем энтропию\n",
    "\n",
    "    def f_entropy(self,p):\n",
    "        \n",
    "        p = np.bincount(p) / float(p.shape[0])\n",
    "\n",
    "        ep = stats.entropy(p)\n",
    "        if ep == -float('inf'):\n",
    "            return 0.0\n",
    "        return ep\n",
    "\n",
    "        # Выводим Прирост информации\n",
    "        \n",
    "    def information_gain(self, y, splits):\n",
    "        splits_entropy = sum([self.f_entropy(split) * (float(split.shape[0]) / y.shape[0]) for split in splits])\n",
    "        return self.f_entropy(y) - splits_entropy\n",
    "\n",
    "        # Делаем сплит по  значению\n",
    "        \n",
    "    def split(self, X, y, value):\n",
    "        left_mask = (X < value)\n",
    "        right_mask = (X >= value)\n",
    "        return y[left_mask], y[right_mask]\n",
    "    \n",
    "    \n",
    "    def get_split_mask(self,X, column, value):\n",
    "        left_mask = (X[:, column] < value)\n",
    "        right_mask = (X[:, column] >= value)\n",
    "        return left_mask, right_mask\n",
    "    \n",
    "    \n",
    "    def split_dataset(self,X, target, column, value, return_X=True):\n",
    "        \n",
    "        left_mask, right_mask = self.get_split_mask(X, column, value)\n",
    "\n",
    "        left, right = [], []\n",
    "        \n",
    "        left = target[left_mask]\n",
    "        right = target[right_mask]\n",
    "\n",
    "        if return_X:\n",
    "            left_X, right_X = X[left_mask], X[right_mask]\n",
    "            return left_X, right_X, left, right\n",
    "        else:\n",
    "            return left, right\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Отбираем все промежуточные значения где можно сделать сплит\n",
    "    def find_splits(self, X):\n",
    "        \"\"\"Find all possible split values.\"\"\"\n",
    "        split_values = set()\n",
    "        x_unique = list(np.unique(X))\n",
    "        \n",
    "        for i in range(1, len(x_unique)):\n",
    "            average = (x_unique[i - 1] + x_unique[i]) / 2.0\n",
    "            split_values.add(average)\n",
    "\n",
    "        return list(split_values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        # Отбираем лучшее разбиение по приросту информации(Чем ближе к энтропии таргета тем лучше)\n",
    "        \n",
    "    def find_best_split(self,X, target, n_features):\n",
    "        \"\"\"Find best feature and value for a split. Greedy algorithm.\"\"\"\n",
    "\n",
    "        # Случайные признаки\n",
    "        subset = random.sample(list(range(0, X.shape[1])), n_features)\n",
    "        max_gain, max_col, max_val = None, None, None\n",
    "\n",
    "        for column in subset:\n",
    "            \n",
    "            # Здесь мы ищем уникальные значения и возвращаем промежутки между ними\n",
    "            split_values = self.find_splits(X[:, column])\n",
    "            \n",
    "            for value in split_values:\n",
    "                    # Разделяем выборку по признаку и значению/ тоже самое и с целевой меткой\n",
    "                    splits = self.split(X[:, column], target, value)\n",
    "                    \n",
    "                    # Считаем Прирост информации для этого разбиения\n",
    "                    gain = self.information_gain(target, splits)\n",
    "                \n",
    "                    # условие останова по приросту информации\n",
    "                    # Если новый gain больше предыдущего то мы продолжаем! искать, если нет то останаваливаемся\n",
    "                    \n",
    "                    if (max_gain is None) or (gain > max_gain):\n",
    "                        \n",
    "                        max_col, max_val, max_gain = column, value, gain\n",
    "                        \n",
    "                        \n",
    "               # Возвращаем колонку значение и прирост информации     \n",
    "        return max_col, max_val, max_gain\n",
    "\n",
    "    def train(self,X, target, max_features=None, min_samples_split=2, max_depth = None):\n",
    "        \"\"\"Build a decision tree from training set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "            Feature dataset.\n",
    "        target : dictionary or array-like\n",
    "            Target values.\n",
    "        max_features : int or None\n",
    "            The number of features to consider when looking for the best split.\n",
    "        min_samples_split : int\n",
    "            The minimum number of samples required to split an internal node.\n",
    "        max_depth : int\n",
    "            Maximum depth of the tree.\n",
    "        minimum_gain : float, default 0.01\n",
    "            Minimum gain required for splitting.\n",
    "        loss : function, default None\n",
    "            Loss function for gradient boosting.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "        # Здесь мы прописываем условия развития дерева\n",
    "        \n",
    "        try:\n",
    "            # Заканчиваем если Кол-во объектов, Максимальная глубина\n",
    "            assert (X.shape[0] > min_samples_split)\n",
    "            \n",
    "            # если глубина не заданна то растем на всю катушку!\n",
    "            if max_depth is not None:\n",
    "                assert (max_depth > 0)\n",
    "                max_depth = max_depth - 1\n",
    "\n",
    "            if max_features is None:\n",
    "                max_features = X.shape[1]\n",
    "                \n",
    "            column, value, gain = self.find_best_split(X, target, max_features)\n",
    "            \n",
    "            \n",
    "            assert gain is not None\n",
    "        \n",
    "\n",
    "            self.column_index = column\n",
    "            self.threshold = value\n",
    "            self.impurity = gain\n",
    "\n",
    "            # Делим датасет по лучшему сплиту\n",
    "            left_X, right_X, left_target, right_target = self.split_dataset(X, target, column, value)\n",
    "\n",
    "            # Создаем Левое и правое дерево! Здесь можно ускорить за счет распаралеривония\n",
    "            \n",
    "            \n",
    "            self.left_child = Tree()\n",
    "            self.left_child.train(left_X, left_target, max_features, min_samples_split, max_depth)\n",
    "\n",
    "            self.right_child = Tree()\n",
    "            self.right_child.train(right_X, right_target, max_features, min_samples_split, max_depth)\n",
    "            \n",
    "        except AssertionError:\n",
    "            self._calculate_leaf_value(target)\n",
    "            \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        # Передаем все параметры в метод трайн\n",
    "        self.train(X, y, self.max_features,self.min_samples_split, self.max_depth)\n",
    "\n",
    "    def _calculate_leaf_value(self, target):\n",
    "        \"\"\"Find optimal value for leaf.\"\"\"\n",
    "        \n",
    "        # Задаем вероятность отнесения к классу в узле\n",
    "        # self.outcome = stats.itemfreq(target)[:, 1] / float(target.shape[0])\n",
    "        \n",
    "        # Определяем Максимально встречающийся класс\n",
    "        \n",
    "        self.outcome = np.max(target)\n",
    "        \n",
    "\n",
    "    def predict_row(self, row):\n",
    "        \"\"\"Predict single row.\"\"\"\n",
    "        #  Если узел то движемся дальше\n",
    "        if not self.is_terminal:\n",
    "            if row[self.column_index] < self.threshold:\n",
    "                return self.left_child.predict_row(row)\n",
    "            else:\n",
    "                return self.right_child.predict_row(row)\n",
    "            \n",
    "        # Если лист то возвращаем максимальный класс\n",
    "        \n",
    "        return self.outcome\n",
    "\n",
    "    def predict(self, X):\n",
    "        result = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            result[i] = self.predict_row(X[i, :])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "# Попробую пока что разобратся на бинарной классификации\n",
    "X = iris.data # petal length and width\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Tree()\n",
    "tree.fit(X,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n",
      "1.0\n",
      "1.0\n",
      "0.9333333333333333\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "gkf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train, test in gkf.split(X, target):\n",
    "    X_train, y_train = X[train], target[train]\n",
    "    X_test, y_test = X[test], target[test]   \n",
    "    tree.fit(X_train, y_train)\n",
    "    print(accuracy_score(tree.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/Users/imac/Desktop/Python/Otus/Lesson8_Future_Eng_Titanick/train.csv', index_col = 'PassengerId')\n",
    "df_test = pd.read_csv('/Users/imac/Desktop/Python/Otus/Lesson8_Future_Eng_Titanick/test.csv', index_col = 'PassengerId')\n",
    "\n",
    "# Объеденим 2 фрейма чтобы можно было преобразовывать спокойно, но запомним индексы\n",
    "\n",
    "idx_train = df_train.shape\n",
    "\n",
    "idx_split = df_train.shape[0]\n",
    "Data = pd.concat([df_train, df_test])\n",
    "# Сразу выделяю целевой класс\n",
    "Y_TRAIN = df_train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, Imputer\n",
    "from sklearn.preprocessing import CategoricalEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class Simple_pipeline(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        # Embarked содержит 2 пропуска! Я решил заполнить их самым частым признаком\n",
    "        # Возможно так делать не корректно но я хотел добавить именно строковый элемент,\n",
    "        # чтобы можно было преобразовать сразу CategoricalEncoder\n",
    "\n",
    "    def embarked_fill(self, df):\n",
    "        max_count_embarked = df['Embarked'].value_counts().index[0]\n",
    "        df['Embarked'] = df['Embarked'].fillna(max_count_embarked)\n",
    "        return df['Embarked'].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "    def get_sex_pclass_col(self, df):\n",
    "        # Pclass,SibSp,Parch уже числовые поэтому CategoricalEncoder для них сделает просто onehot\n",
    "        return df[['Sex','Pclass','SibSp','Parch']]\n",
    "\n",
    "    def get_num_cols(self, df):\n",
    "        return df[['Age', 'Fare']]\n",
    "    \n",
    "    def get_pipline(self):\n",
    "\n",
    "        pipeline = make_union(*[\n",
    "            make_pipeline(FunctionTransformer(self.get_sex_pclass_col, validate=False), \n",
    "                          CategoricalEncoder(encoding='ordinal')),\n",
    "            \n",
    "            make_pipeline(FunctionTransformer(self.embarked_fill, validate=False),\n",
    "                          CategoricalEncoder(encoding='ordinal')), \n",
    "\n",
    "            make_pipeline(FunctionTransformer(self.get_num_cols, validate=False), Imputer(strategy='mean'),StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        return pipeline.fit_transform(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "X_Data_full = Simple_pipeline(Data).get_pipline()\n",
    "x_train_full, x_valid_full, y_train_full, y_valid_full = train_test_split(X_Data_full[:idx_split], Y_TRAIN,\n",
    "                                                    test_size=0.3, stratify=Y_TRAIN, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titanic_tree = Tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titanic_tree.fit(x_train_full,y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7686567164179104\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(Titanic_tree.predict(x_valid_full),y_valid_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7723880597014925\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(x_train_full,y_train_full)\n",
    "print(accuracy_score(y_pred=clf.predict(x_valid_full), y_true=y_valid_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
